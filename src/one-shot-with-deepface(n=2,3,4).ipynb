{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32edc4b3-3a67-4dee-99cf-ee7d3cfe6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to preprocess the images\n",
    "def preprocess_image(img_path, target_size=(120, 120)):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing it and normalizing it between 0 and 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path: str, path to the image file.\n",
    "    - target_size: tuple, target size for resizing.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img = img_to_array(img) / 255.0  # Normalize the image to range [0, 1]\n",
    "    return img\n",
    "\n",
    "# Function to generate support and query sets for people with at least 2 images\n",
    "def create_support_query_sets(dataset_path, person_folders, images_by_person, num_support=1):\n",
    "    \"\"\"\n",
    "    Create support and query sets for each person based on available images.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: str, path to the dataset containing the person folders.\n",
    "    - person_folders: list of str, list of person names or folder names.\n",
    "    - images_by_person: dict, mapping of person to list of their image paths.\n",
    "    - num_support: int, number of support images to pick for each person (default 1).\n",
    "\n",
    "    Returns:\n",
    "    - support_set: dict, mapping of person to their support image(s).\n",
    "    - query_set: dict, mapping of person to their query image(s).\n",
    "    \"\"\"\n",
    "    support_set = {}\n",
    "    query_set = {}\n",
    "\n",
    "    # Loop over each person\n",
    "    for person in person_folders:\n",
    "        images = images_by_person.get(person, [])\n",
    "        \n",
    "        # Consider people with at least 2 images\n",
    "        if len(images)==2:\n",
    "            support_images = random.sample(images, num_support)  # Pick images for support\n",
    "            query_images = [img for img in images if img not in support_images]  # Remaining images for query\n",
    "\n",
    "            support_set[person] = support_images\n",
    "            query_set[person] = query_images\n",
    "\n",
    "    return support_set, query_set\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `dataset_path` is the directory containing subdirectories for each person,\n",
    "# and `images_by_person` is a dictionary that maps each person to their list of image paths.\n",
    "\n",
    "# Example: Dataset structure\n",
    "dataset_path = '/Users/shuchanzhou/Downloads/archive-2/lfw-deepfunneled/lfw-deepfunneled'\n",
    "\n",
    "person_folders = os.listdir(dataset_path)  # Get all person folders in the dataset\n",
    "\n",
    "# Create a dictionary mapping each person to their list of image paths\n",
    "images_by_person = {}\n",
    "for person in person_folders:\n",
    "    person_folder_path = os.path.join(dataset_path, person)\n",
    "    if os.path.isdir(person_folder_path):\n",
    "        images = [os.path.join(person_folder_path, img) for img in os.listdir(person_folder_path) if img.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        images_by_person[person] = images\n",
    "\n",
    "# Now, create the support and query sets\n",
    "support_set, query_set = create_support_query_sets(dataset_path, person_folders, images_by_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360f651b-7da3-429f-b8ad-9102db9d014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_embeddings1(query_embedding, support_embedding):\n",
    "    \"\"\"\n",
    "    Compare the query embedding with support embeddings and return the cosine distance score.\n",
    "\n",
    "    Parameters:\n",
    "    - query_embedding: The embedding of the query image\n",
    "    - support_embedding: The embedding of a support image\n",
    "    \n",
    "    Returns:\n",
    "    - Cosine distance between the two embeddings (lower distance means more similar)\n",
    "    \"\"\"\n",
    "    cosine_sim = cosine_similarity([query_embedding], [support_embedding])[0][0]\n",
    "    cosine_dist = 1 - cosine_sim  # Convert similarity to distance\n",
    "    return cosine_dist\n",
    "\n",
    "\n",
    "\n",
    "def compare_embeddings(query_embedding, support_embedding):\n",
    "    \"\"\"\n",
    "    Compare the query embedding with support embeddings and return the similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - query_embedding: The embedding of the query image\n",
    "    - support_embedding: The embedding of a support image\n",
    "    \n",
    "    Returns:\n",
    "    - Distance between the two embeddings (lower distance means more similar)\n",
    "    \"\"\"\n",
    "    return euclidean_distances([query_embedding], [support_embedding])[0][0]\n",
    "    \n",
    "\n",
    "def few_shot_inference(support_embeddings, query_embeddings):\n",
    "    \"\"\"\n",
    "    Perform few-shot learning inference by comparing query images with the support set.\n",
    "\n",
    "    Parameters:\n",
    "    - support_embeddings: Dictionary of support embeddings for each person\n",
    "    - query_embeddings: Dictionary of query embeddings for each person\n",
    "\n",
    "    Returns:\n",
    "    - correct_predictions: Number of correct predictions\n",
    "    - total_predictions: Total number of predictions made\n",
    "    - accuracy: Accuracy of the model on the query set\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for person in query_embeddings:\n",
    "        for query_embedding in query_embeddings[person]:\n",
    "            min_distance = float('inf')\n",
    "            predicted_person = None\n",
    "\n",
    "            # Compare query image with all support images (from all people)\n",
    "            for support_person, support_images in support_embeddings.items():\n",
    "                for support_embedding in support_images:\n",
    "                    # Compare query embedding with support embedding\n",
    "                    distance = compare_embeddings1(query_embedding, support_embedding)\n",
    "                \n",
    "\n",
    "                    # If smaller distance, update prediction\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        predicted_person = support_person\n",
    "\n",
    "            # Check if the prediction matches the actual person\n",
    "            if predicted_person == person:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy, correct_predictions, total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff5303b-76c3-4a00-995b-9bcc5b96f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "\n",
    "def generate_embeddings_for_few_shot(model, support_set, query_set):\n",
    "    \"\"\"\n",
    "    Generate embeddings for both the support and query sets.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model used for face verification (e.g., 'Facenet', 'VGG-Face', etc.)\n",
    "    - support_set: Dictionary mapping person to their support images\n",
    "    - query_set: Dictionary mapping person to their query images\n",
    "\n",
    "    Returns:\n",
    "    - support_embeddings: Dictionary of embeddings for the support images\n",
    "    - query_embeddings: Dictionary of embeddings for the query images\n",
    "    \"\"\"\n",
    "    support_embeddings = {}\n",
    "    query_embeddings = {}\n",
    "    \n",
    "    # Generate embeddings for the support set\n",
    "    for person, images in support_set.items():\n",
    "        embeddings = []\n",
    "        for img in images:\n",
    "            result = DeepFace.represent(img, model_name=model,enforce_detection=False)\n",
    "            embedding = result[0]['embedding']  # Extract embedding from the result\n",
    "            embeddings.append(embedding)\n",
    "        support_embeddings[person] = embeddings\n",
    "        \n",
    "\n",
    "    # Generate embeddings for the query set\n",
    "    for person, images in query_set.items():\n",
    "        embeddings = []\n",
    "        for img in images:\n",
    "            result = DeepFace.represent(img, model_name=model,enforce_detection=False)\n",
    "            embedding = result[0]['embedding']  # Extract embedding from the result\n",
    "            embeddings.append(embedding)\n",
    "        query_embeddings[person] = embeddings\n",
    "\n",
    "    return support_embeddings, query_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec1fa9-a231-406e-852b-d2c555eaa6d3",
   "metadata": {},
   "source": [
    "## one shot for n=2; 779 pictures in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4069e1-642d-4d13-b7f1-5881904b4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.50%\n",
      "Correct Predictions: 518/779\n"
     ]
    }
   ],
   "source": [
    "model = 'Facenet'  # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a71a2c9-02ce-432b-8c0d-e30299a8628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.91%\n",
      "Correct Predictions: 568/779\n"
     ]
    }
   ],
   "source": [
    "model = 'Facenet512'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c10a7e-626c-4e59-98a0-498c76bf1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-30 17:05:45 - Pre-trained weights is downloaded from https://github.com/HamadYA/GhostFaceNets/releases/download/v1.2/GhostFaceNet_W1.3_S1_ArcFace.h5 to /Users/shuchanzhou/.deepface/weights/ghostfacenet_v1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/HamadYA/GhostFaceNets/releases/download/v1.2/GhostFaceNet_W1.3_S1_ArcFace.h5\n",
      "To: /Users/shuchanzhou/.deepface/weights/ghostfacenet_v1.h5\n",
      "100%|█████████████████████████████████████████████████████████████████| 17.3M/17.3M [00:00<00:00, 17.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-30 17:05:47 - Pre-trained weights is just downloaded to /Users/shuchanzhou/.deepface/weights/ghostfacenet_v1.h5\n",
      "Accuracy: 65.34%\n",
      "Correct Predictions: 509/779\n"
     ]
    }
   ],
   "source": [
    "model = 'GhostFaceNet'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6964b2c0-93ea-4b52-b76d-0b54986915f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-30 17:12:29 - openface_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/openface_weights.h5\n",
      "To: /Users/shuchanzhou/.deepface/weights/openface_weights.h5\n",
      "100%|█████████████████████████████████████████████████████████████████| 15.3M/15.3M [00:00<00:00, 19.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6.42%\n",
      "Correct Predictions: 50/779\n"
     ]
    }
   ],
   "source": [
    "model = 'OpenFace'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0235c8-f895-492a-bc85-8c4b35810ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-30 17:16:20 - deepid_keras_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/deepid_keras_weights.h5\n",
      "To: /Users/shuchanzhou/.deepface/weights/deepid_keras_weights.h5\n",
      "100%|█████████████████████████████████████████████████████████████████| 1.61M/1.61M [00:00<00:00, 4.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6.03%\n",
      "Correct Predictions: 47/779\n"
     ]
    }
   ],
   "source": [
    "model = 'DeepID'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3569db04-e954-4718-81ae-b403e11390b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-30 17:19:51 - face_recognition_sface_2021dec.onnx weights will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/opencv/opencv_zoo/raw/main/models/face_recognition_sface/face_recognition_sface_2021dec.onnx\n",
      "To: /Users/shuchanzhou/.deepface/weights/face_recognition_sface_2021dec.onnx\n",
      "100%|█████████████████████████████████████████████████████████████████| 38.7M/38.7M [00:01<00:00, 27.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.03%\n",
      "Correct Predictions: 491/779\n"
     ]
    }
   ],
   "source": [
    "model = 'SFace'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b871d-fd63-41dd-847a-de9e4fd9527b",
   "metadata": {},
   "source": [
    "## one shot for n=2,3; 1261 pictures in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8caf6c8f-d95d-4997-8f87-e823d54872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to preprocess the images\n",
    "def preprocess_image(img_path, target_size=(120, 120)):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing it and normalizing it between 0 and 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path: str, path to the image file.\n",
    "    - target_size: tuple, target size for resizing.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img = img_to_array(img) / 255.0  # Normalize the image to range [0, 1]\n",
    "    return img\n",
    "\n",
    "# Function to generate support and query sets for people with at least 2 images\n",
    "def create_support_query_sets(dataset_path, person_folders, images_by_person, num_support=1):\n",
    "    \"\"\"\n",
    "    Create support and query sets for each person based on available images.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: str, path to the dataset containing the person folders.\n",
    "    - person_folders: list of str, list of person names or folder names.\n",
    "    - images_by_person: dict, mapping of person to list of their image paths.\n",
    "    - num_support: int, number of support images to pick for each person (default 1).\n",
    "\n",
    "    Returns:\n",
    "    - support_set: dict, mapping of person to their support image(s).\n",
    "    - query_set: dict, mapping of person to their query image(s).\n",
    "    \"\"\"\n",
    "    support_set = {}\n",
    "    query_set = {}\n",
    "\n",
    "    # Loop over each person\n",
    "    for person in person_folders:\n",
    "        images = images_by_person.get(person, [])\n",
    "        \n",
    "        # Consider people with at least 2 images\n",
    "        if len(images)==2 or len(images)==3:\n",
    "            support_images = random.sample(images, num_support)  # Pick images for support\n",
    "            query_images = [img for img in images if img not in support_images]  # Remaining images for query\n",
    "\n",
    "            support_set[person] = support_images\n",
    "            query_set[person] = query_images\n",
    "\n",
    "    return support_set, query_set\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `dataset_path` is the directory containing subdirectories for each person,\n",
    "# and `images_by_person` is a dictionary that maps each person to their list of image paths.\n",
    "\n",
    "# Example: Dataset structure\n",
    "dataset_path = '/Users/shuchanzhou/Downloads/archive-2/lfw-deepfunneled/lfw-deepfunneled'\n",
    "\n",
    "person_folders = os.listdir(dataset_path)  # Get all person folders in the dataset\n",
    "\n",
    "# Create a dictionary mapping each person to their list of image paths\n",
    "images_by_person = {}\n",
    "for person in person_folders:\n",
    "    person_folder_path = os.path.join(dataset_path, person)\n",
    "    if os.path.isdir(person_folder_path):\n",
    "        images = [os.path.join(person_folder_path, img) for img in os.listdir(person_folder_path) if img.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        images_by_person[person] = images\n",
    "\n",
    "# Now, create the support and query sets\n",
    "support_set, query_set = create_support_query_sets(dataset_path, person_folders, images_by_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6850c128-cdb6-4a49-b3a2-a0ff9264cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_embeddings1(query_embedding, support_embedding):\n",
    "    \"\"\"\n",
    "    Compare the query embedding with support embeddings and return the cosine distance score.\n",
    "\n",
    "    Parameters:\n",
    "    - query_embedding: The embedding of the query image\n",
    "    - support_embedding: The embedding of a support image\n",
    "    \n",
    "    Returns:\n",
    "    - Cosine distance between the two embeddings (lower distance means more similar)\n",
    "    \"\"\"\n",
    "    cosine_sim = cosine_similarity([query_embedding], [support_embedding])[0][0]\n",
    "    cosine_dist = 1 - cosine_sim  # Convert similarity to distance\n",
    "    return cosine_dist\n",
    "\n",
    "\n",
    "\n",
    "def compare_embeddings(query_embedding, support_embedding):\n",
    "    \"\"\"\n",
    "    Compare the query embedding with support embeddings and return the similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - query_embedding: The embedding of the query image\n",
    "    - support_embedding: The embedding of a support image\n",
    "    \n",
    "    Returns:\n",
    "    - Distance between the two embeddings (lower distance means more similar)\n",
    "    \"\"\"\n",
    "    return euclidean_distances([query_embedding], [support_embedding])[0][0]\n",
    "    \n",
    "\n",
    "def few_shot_inference(support_embeddings, query_embeddings):\n",
    "    \"\"\"\n",
    "    Perform few-shot learning inference by comparing query images with the support set.\n",
    "\n",
    "    Parameters:\n",
    "    - support_embeddings: Dictionary of support embeddings for each person\n",
    "    - query_embeddings: Dictionary of query embeddings for each person\n",
    "\n",
    "    Returns:\n",
    "    - correct_predictions: Number of correct predictions\n",
    "    - total_predictions: Total number of predictions made\n",
    "    - accuracy: Accuracy of the model on the query set\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for person in query_embeddings:\n",
    "        for query_embedding in query_embeddings[person]:\n",
    "            min_distance = float('inf')\n",
    "            predicted_person = None\n",
    "\n",
    "            # Compare query image with all support images (from all people)\n",
    "            for support_person, support_images in support_embeddings.items():\n",
    "                for support_embedding in support_images:\n",
    "                    # Compare query embedding with support embedding\n",
    "                    distance = compare_embeddings1(query_embedding, support_embedding)\n",
    "                \n",
    "\n",
    "                    # If smaller distance, update prediction\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        predicted_person = support_person\n",
    "\n",
    "            # Check if the prediction matches the actual person\n",
    "            if predicted_person == person:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy, correct_predictions, total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49727ee-b94d-41ed-a025-c922f0c705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "\n",
    "def generate_embeddings_for_few_shot(model, support_set, query_set):\n",
    "    \"\"\"\n",
    "    Generate embeddings for both the support and query sets.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model used for face verification (e.g., 'Facenet', 'VGG-Face', etc.)\n",
    "    - support_set: Dictionary mapping person to their support images\n",
    "    - query_set: Dictionary mapping person to their query images\n",
    "\n",
    "    Returns:\n",
    "    - support_embeddings: Dictionary of embeddings for the support images\n",
    "    - query_embeddings: Dictionary of embeddings for the query images\n",
    "    \"\"\"\n",
    "    support_embeddings = {}\n",
    "    query_embeddings = {}\n",
    "    \n",
    "    # Generate embeddings for the support set\n",
    "    for person, images in support_set.items():\n",
    "        embeddings = []\n",
    "        for img in images:\n",
    "            result = DeepFace.represent(img, model_name=model,enforce_detection=False)\n",
    "            embedding = result[0]['embedding']  # Extract embedding from the result\n",
    "            embeddings.append(embedding)\n",
    "        support_embeddings[person] = embeddings\n",
    "        \n",
    "\n",
    "    # Generate embeddings for the query set\n",
    "    for person, images in query_set.items():\n",
    "        embeddings = []\n",
    "        for img in images:\n",
    "            result = DeepFace.represent(img, model_name=model,enforce_detection=False)\n",
    "            embedding = result[0]['embedding']  # Extract embedding from the result\n",
    "            embeddings.append(embedding)\n",
    "        query_embeddings[person] = embeddings\n",
    "\n",
    "    return support_embeddings, query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9870191-5ea7-457d-a04b-01b20ef846b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.51%\n",
      "Correct Predictions: 946/1361\n"
     ]
    }
   ],
   "source": [
    "model = 'Facenet512'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fc0e9-e1c3-4886-bd06-b44ef41e5570",
   "metadata": {},
   "source": [
    "## one shot for n=2,3,4; 1922 pictures in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2203628e-972e-4589-b8af-998a030077de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to preprocess the images\n",
    "def preprocess_image(img_path, target_size=(120, 120)):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing it and normalizing it between 0 and 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path: str, path to the image file.\n",
    "    - target_size: tuple, target size for resizing.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img = img_to_array(img) / 255.0  # Normalize the image to range [0, 1]\n",
    "    return img\n",
    "\n",
    "# Function to generate support and query sets for people with at least 2 images\n",
    "def create_support_query_sets(dataset_path, person_folders, images_by_person, num_support=1):\n",
    "    \"\"\"\n",
    "    Create support and query sets for each person based on available images.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: str, path to the dataset containing the person folders.\n",
    "    - person_folders: list of str, list of person names or folder names.\n",
    "    - images_by_person: dict, mapping of person to list of their image paths.\n",
    "    - num_support: int, number of support images to pick for each person (default 1).\n",
    "\n",
    "    Returns:\n",
    "    - support_set: dict, mapping of person to their support image(s).\n",
    "    - query_set: dict, mapping of person to their query image(s).\n",
    "    \"\"\"\n",
    "    support_set = {}\n",
    "    query_set = {}\n",
    "\n",
    "    # Loop over each person\n",
    "    for person in person_folders:\n",
    "        images = images_by_person.get(person, [])\n",
    "        \n",
    "        # Consider people with at least 2 images\n",
    "        if len(images)==2 or len(images)==3 or len(images)==4:\n",
    "            support_images = random.sample(images, num_support)  # Pick images for support\n",
    "            query_images = [img for img in images if img not in support_images]  # Remaining images for query\n",
    "\n",
    "            support_set[person] = support_images\n",
    "            query_set[person] = query_images\n",
    "\n",
    "    return support_set, query_set\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `dataset_path` is the directory containing subdirectories for each person,\n",
    "# and `images_by_person` is a dictionary that maps each person to their list of image paths.\n",
    "\n",
    "# Example: Dataset structure\n",
    "dataset_path = '/Users/shuchanzhou/Downloads/archive-2/lfw-deepfunneled/lfw-deepfunneled'\n",
    "\n",
    "person_folders = os.listdir(dataset_path)  # Get all person folders in the dataset\n",
    "\n",
    "# Create a dictionary mapping each person to their list of image paths\n",
    "images_by_person = {}\n",
    "for person in person_folders:\n",
    "    person_folder_path = os.path.join(dataset_path, person)\n",
    "    if os.path.isdir(person_folder_path):\n",
    "        images = [os.path.join(person_folder_path, img) for img in os.listdir(person_folder_path) if img.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        images_by_person[person] = images\n",
    "\n",
    "# Now, create the support and query sets\n",
    "support_set, query_set = create_support_query_sets(dataset_path, person_folders, images_by_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403e11be-ec00-43f0-a08c-4392453eed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.85%\n",
      "Correct Predictions: 1304/1922\n"
     ]
    }
   ],
   "source": [
    "model = 'Facenet512'   # or any other model name that DeepFace supports\n",
    "\n",
    "# Generate embeddings for both support and query sets\n",
    "support_embeddings, query_embeddings = generate_embeddings_for_few_shot(model, support_set, query_set)\n",
    "\n",
    "# Perform few-shot learning and get accuracy\n",
    "accuracy, correct_predictions, total_predictions = few_shot_inference(support_embeddings, query_embeddings)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5049ba-efea-46a9-9f91-9d9e734f22c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
